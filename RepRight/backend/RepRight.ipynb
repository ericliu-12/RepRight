{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0MH3oqgYsUA8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize mediapipe pose estimation model\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get keypoints from image with mediapipe\n",
        "def extract_keypoints(image):\n",
        "    # convert image to RGB\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(rgb_image)\n",
        "\n",
        "    # if no landmarks (joints) return zeros\n",
        "    if not results.pose_landmarks:\n",
        "        return np.zeros(33*3)\n",
        "    \n",
        "    # get keypoints in (x,y,z) coordinates format\n",
        "    keypoints = []\n",
        "    for landmark in results.pose_landmarks.landmark:\n",
        "        keypoints.append([landmark.x, landmark.y, landmark.z])\n",
        "    return np.array(keypoints).flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load images and their labels\n",
        "def load_images(folder):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # go through each folder\n",
        "    for exercise in os.listdir(folder):\n",
        "        exercise_folder = os.path.join(folder,exercise)\n",
        "        if os.path.isdir(exercise_folder):\n",
        "            # go through each image in folder\n",
        "            for img_file in os.listdir(exercise_folder):\n",
        "                img_path = os.path.join(exercise_folder, img_file)\n",
        "\n",
        "                # read image\n",
        "                image = cv2.imread(img_path)\n",
        "                if image is not None:\n",
        "                    keypoints = extract_keypoints(image)\n",
        "                    data.append(keypoints)\n",
        "                    labels.append(exercise)\n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Vince\\AppData\\Roaming\\Python\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        }
      ],
      "source": [
        "# load images from workout data folder\n",
        "base_folder = \"./workout_data\"\n",
        "data,labels = load_images(base_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encode exercise names into numerical format\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainData, testData, trainLabel, testLabel = train_test_split(data, labels_encoded, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Vince\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# build neural network to classify exercises based on keypoints\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation=\"relu\", input_shape=(trainData.shape[1],)),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(len(np.unique(labels_encoded)), activation=\"softmax\")\n",
        "])\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6733 - loss: 0.7885 - val_accuracy: 0.8174 - val_loss: 0.4430\n",
            "Epoch 2/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8494 - loss: 0.4036 - val_accuracy: 0.8447 - val_loss: 0.3634\n",
            "Epoch 3/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8928 - loss: 0.3180 - val_accuracy: 0.8653 - val_loss: 0.3353\n",
            "Epoch 4/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8925 - loss: 0.2976 - val_accuracy: 0.8950 - val_loss: 0.2621\n",
            "Epoch 5/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9064 - loss: 0.2363 - val_accuracy: 0.9178 - val_loss: 0.2280\n",
            "Epoch 6/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9311 - loss: 0.2179 - val_accuracy: 0.9132 - val_loss: 0.2019\n",
            "Epoch 7/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9284 - loss: 0.1979 - val_accuracy: 0.9201 - val_loss: 0.1940\n",
            "Epoch 8/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9381 - loss: 0.1765 - val_accuracy: 0.9315 - val_loss: 0.1870\n",
            "Epoch 9/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9527 - loss: 0.1498 - val_accuracy: 0.9338 - val_loss: 0.1660\n",
            "Epoch 10/10\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9548 - loss: 0.1382 - val_accuracy: 0.9429 - val_loss: 0.1664\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x23000465870>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(trainData, trainLabel, epochs=10, validation_data=(testData, testLabel))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9526 - loss: 0.1662\n",
            "test accuracy: 0.9429223537445068\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(testData, testLabel)\n",
        "print(f\"test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use trained model to predict on new images\n",
        "def classify_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    keypoints = extract_keypoints(image)\n",
        "    keypoints = np.expand_dims(keypoints, axis=0)\n",
        "    prediction = model.predict(keypoints)\n",
        "    predicted_class = label_encoder.inverse_transform([np.argmax(prediction)])\n",
        "    return predicted_class[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_video(video_path):\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "    while vid.isOpened():\n",
        "        ret, frame = vid.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        keypoints = extract_keypoints(frame)\n",
        "        keypoints = np.expand_dims(keypoints, axis=0)\n",
        "        prediction = model.predict(keypoints)\n",
        "        predicted_class = label_encoder.inverse_transform([np.argmax(prediction)])\n",
        "        cv2.putText(frame, f'Predicted: {predicted_class[0]}', (10, 30), \n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "        \n",
        "        cv2.imshow(\"Video classification\", frame)\n",
        "\n",
        "        # break loop on 'q' key press\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    vid.release()\n",
        "    cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "ename": "error",
          "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[88], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path_test_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./eric_pushup.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m predicted_exercise \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_test_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted exercise: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_exercise\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[36], line 4\u001b[0m, in \u001b[0;36mclassify_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_image\u001b[39m(image_path):\n\u001b[0;32m      3\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m----> 4\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(keypoints, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(keypoints)\n",
            "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36mextract_keypoints\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_keypoints\u001b[39m(image):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# convert image to RGB\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     rgb_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(rgb_image)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# if no landmarks (joints) return zeros\u001b[39;00m\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
          ]
        }
      ],
      "source": [
        "path_test_img = \"./eric_pushup.mp4\"\n",
        "predicted_exercise = classify_video(path_test_img)\n",
        "print(f\"predicted exercise: {predicted_exercise}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
